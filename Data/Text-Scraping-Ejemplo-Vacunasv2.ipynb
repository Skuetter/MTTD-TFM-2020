{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ejemplo de Text-Scraping para Dani!!</h2>\n",
    "<p>Hola Dani, esta ma&ntilde;ana me ha dado por crearte un ejemplo r&aacute;pido y sucio (<span data-dobid=\"hdw\">quick and dirty)</span>&nbsp;para ense&ntilde;arte como hacer un Text scraping. Para ello vamos a 'scrapear' la p&aacute;gina:</p>\n",
    "<p>&nbsp;<a href=\"https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/salud/faq.htm\">https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/salud/faq.htm</a></p>\n",
    "<p>&nbsp;</p>\n",
    "<p>Que contiene las y preguntas y respuestas m&aacute;s habituales en un Servicio de Vacunaci&oacute;n Internacional&nbsp;</p>\n",
    "<p>Con ellas vamos a crear una tabla</p>\n",
    "<p> Son pocas preguntas (50) y seguramente con un copy paste iríamos más rapido, pero se trata simplemente un ejemplo ilustrativo</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Importamos las librerias que necesitamos para el scraping</p>\n",
    "<ul>\n",
    "    <li> Requests: para hacer los http requests</li>\n",
    "    <li> BeutifulSoup: para hacer el scraping</li>\n",
    "    <li>pickle: para guardar la información</li>\n",
    "</ul>\n",
    "\n",
    "<p>definimos una función dónde le damos una URL y extraemos la información de la clase \"informacion\" y de los elementos h2, h3...</p> \n",
    "<p> Esto lo hacemos tras analizar la página y ver dónde se encuentra la información que necesitamos!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "url = 'https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/salud/faq.htm'\n",
    "\n",
    "page = requests.get(url,verify=False).text\n",
    "soup = BeautifulSoup(page,\"html.parser\")\n",
    "\n",
    "text = [p.text for p in soup.find (class_=\"informacion\").find_all([\"h2\",\"h3\", \"p\", \"ol\",\"ul\", \"li\"])]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing last element of the list! we don't need the text: Si desea localizar información... !!!\n",
    "text.pop()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Guardamos y exportamos la informaci&oacute;n en bruto por si queremos analizar otro dia más cosas</h3>\n",
    "<p>Para ello utilizamos pickle</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we save the raw data using pickle\n",
    "with open(\"preguntas-gob.txt\", \"wb\") as file:\n",
    "    data=pickle.dump(text,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the pickle\n",
    "with open (\"preguntas-gob.txt\", \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos la información para poder analizar todo el texto como si fuera uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = ' '.join(data)\n",
    "\n",
    "combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>En el texto nos encontramos cosas raras como:</p>\n",
    "<p>'<em> siguiente enlace del Ministerio de Asuntos Exteriores y Cooperaci&oacute;n. \\nSi Viajas al extranjero (Ministerio de Asuntos Exteriores y Cooperaci&oacute;n)<strong>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t \\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n</strong> &iquest;Qu&eacute; debemos tener en cuenta de la Asistencia sanitaria en el extranjero? En los pa&iacute;ses de la Comunidad Europea, la Tarjeta Sanitaria Europea es el documento emitido por el Instituto Nacional de la Seguridad Social (INSS)</em> '</p>\n",
    "<p>Que queremos eliminar!</p>\n",
    "\n",
    "<p>Para ello utilzamos 'expresiones regulares (regex)' </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r'[\\n\\t]')\n",
    "plaintext= regex.sub(\" \", combined_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Una vez tenemos el texto limpio lo queremos separar por pregunta - respuesta </p>\n",
    "<p> para ello creamos una lista y añadimos un nuevo elemento cada vez que nos encontremos \"¿\"</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers= [\"¿\"+ e for e in plaintext.split(\"¿\") if e]\n",
    "questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Aqui me he complicado un poco, pero simplemente:<p>\n",
    "    <ol>\n",
    "        <li>Separamos preguntas de respuestas</li>\n",
    "        <li>Las metemos en un diccionario</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_dicc = {}\n",
    "for question in questions_answers:\n",
    "    pr = question.split('?')\n",
    "    question_dicc [questions_answers.index(question)] = [pr[0]+'?', pr[1].strip()]\n",
    "\n",
    "\n",
    "question_dicc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Para visualizarlo creamos un Dataframe de Pandas</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 3000)\n",
    "data_df = pd.DataFrame.from_dict(question_dicc).transpose()\n",
    "data_df.columns = ['questions', 'answers']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Exportamos la tabla en un CSV!! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(r'preguntas-respuestas-gob.csv',index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Segunda Parte!!</h2>\n",
    "\n",
    "En esta segunda parte vamos a ver un ejemplo de PDF-Scraping!\n",
    "\n",
    "vamos a scrapear algunos pdfs de \n",
    "https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/salud/infVacunas.htm\n",
    "\n",
    "para ello vamos a instalar pdfminer! \n",
    "\n",
    "Todo el notebook se ha realizado de forma rápida para mostrar un ejemplo! Si hay mejoras, porfavor! no dudeis en mejorarlo :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una funcion para guardar los pdfs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    " \n",
    "def save_pdf_from_url(url,pathname):    \n",
    "    response = requests.get(url,verify=False, stream=True)\n",
    "    with open(pathname, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y definimos las urls y guardamos los pdfs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/docs/COLERA.pdf\"\n",
    "pathname1=\"COLERA.pdf\"\n",
    "\n",
    "url2 = \"https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/docs/ENCEFALITIS_CENTRO_EUROPEA_.pdf\"\n",
    "pathname2=\"ENCEFALITIS_CENTRO_EUROPEA.pdf\"\n",
    "\n",
    "url3 = \"https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/docs/FIEBRE_AMARILLA.pdf\"\n",
    "pathname3=\"FIEBRE_AMARILLA.pdf\"\n",
    "\n",
    "url4 = \"https://www.mscbs.gob.es/profesionales/saludPublica/sanidadExterior/docs/FIEBRE_TIFOIDEA_INYECTABLE.pdf\"\n",
    "pathname4=\"FIEBRE_TIFOIDEA_INYECTABLE.pdf\"\n",
    "\n",
    "\n",
    "urls = [[url1,pathname1],[url2,pathname2],[url3,pathname3],[url4,pathname4] ]\n",
    "\n",
    "for url in urls:    \n",
    "    save_pdf_from_url(url[0],url[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para extraer el texto de los pdfs:\n",
    "(reutilizo el ejemplo de https://stackoverflow.com/questions/5725278/how-do-i-use-pdfminer-as-a-library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos el texto de los PDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist = []\n",
    "\n",
    "for url in urls:\n",
    "    print(url[1])\n",
    "    textlist.append((url[1].replace(\".pdf\",\"\"), convert_pdf_to_txt(url[1])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos un poco el texto, aquí se han hecho varias iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(rgx_list, text):\n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, '', new_text)\n",
    "    return new_text.strip()\n",
    "\n",
    "to_replace_1 = r\"Información para el Viajero\\nInformación para el Viajero\\nEncefalitis Centro-Europea\\n\\n\"\n",
    "to_replace_2= r\"\\n\\n     Consulte con un\\nCentro de\\nVacunación\\nInternacional\\n\\nwww.msssi.gob.es/cvi\\n\\n901 400 100\\n\\n\\x0c\"\n",
    "to_replace_3=r\"\\n\"\n",
    "to_replace_4=r\"\\x0c\"\n",
    "to_replace_5=r\"Consulte con un\\n\\nCentro de\\nVacunación\\nInternacional\\n\\nwww.msssi.gob.es/cvi\\n\\n901 400 100\"\n",
    "to_replace_6=r\"(«.*?»)\"\n",
    "to_replace_7=r\"(«.*?«)\"\n",
    "to_replace_8=r\"VacunaCólera     Consulte con unCentro deVacunaciónInternacionalwww.msssi.gob.es/cvi901 400 100\"\n",
    "to_replace_9=r\"Información para el ViajeroInformación para el ViajeroFiebre Amarilla\"\n",
    "\n",
    "#to_replace=[to_replace_1,to_replace_2,to_replace_3,to_replace_4]\n",
    "to_replace=[to_replace_1,to_replace_2,to_replace_3,to_replace_4,to_replace_5,to_replace_6,to_replace_7,\n",
    "           to_replace_8,to_replace_9]\n",
    "\n",
    "\n",
    "cleaned_list =[]\n",
    "\n",
    "for t in textlist:\n",
    "    print(t[1])\n",
    "    cleaned_list.append([t[0],clean_text(to_replace,t[1])])\n",
    "    \n",
    "\n",
    "cleaned_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funcion definida, hace lo que se ha hecho en la parte 1:\n",
    "    separamos preguntas y respuestas y creamos un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do what i've done\n",
    "\n",
    "def prepare_dicc(text):\n",
    "    counter=0\n",
    "    questions_answers_dict = dict()\n",
    "    answers=[]\n",
    "    answers= [\"¿\"+ e for e in text.split(\"¿\") if e]\n",
    "    \n",
    "    for question in answers:\n",
    "        pr = question.split('?')\n",
    "        #I'm cleaning the answers to be sure we don't get some sound/mierda\n",
    "        ans= clean_text(to_replace,pr[1].strip())\n",
    "        questions_answers_dict [counter] = [pr[0]+'?', ans]\n",
    "        counter=counter+1\n",
    "    return questions_answers_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos que preguntas hay en el primer elemento de la lista para crear un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_1 = cleaned_list[0][0]\n",
    "preguntas_1=cleaned_list[0][1]\n",
    "#identifico que voy a ver\n",
    "nombre_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colera_dict = prepare_dicc(preguntas_1)\n",
    "colera_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y por último, utilizamos Pandas para ver la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 3000)\n",
    "data_df = pd.DataFrame.from_dict(colera_dict).transpose()\n",
    "data_df.columns = ['questions', 'answers']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_2 = cleaned_list[1][0]\n",
    "preguntas_2=cleaned_list[1][1]\n",
    "\n",
    "ENCEFALITIS_CENTRO_EUROPEA_dict = prepare_dicc(preguntas_2)\n",
    "\n",
    "pd.set_option('max_colwidth', 3000)\n",
    "data_df = pd.DataFrame.from_dict(ENCEFALITIS_CENTRO_EUROPEA_dict).transpose()\n",
    "data_df.columns = ['questions', 'answers']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_3 = cleaned_list[2][0]\n",
    "preguntas_3=cleaned_list[2][1]\n",
    "\n",
    "FIEBRE_AMARILLA_dict = prepare_dicc(preguntas_3)\n",
    "\n",
    "pd.set_option('max_colwidth', 3000)\n",
    "data_df = pd.DataFrame.from_dict(FIEBRE_AMARILLA_dict).transpose()\n",
    "data_df.columns = ['questions', 'answers']\n",
    "data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_4 = cleaned_list[3][0]\n",
    "preguntas_4=cleaned_list[3][1]\n",
    "\n",
    "FIEBRE_TIFOIDEA_dict = prepare_dicc(preguntas_4)\n",
    "\n",
    "pd.set_option('max_colwidth', 3000)\n",
    "data_df = pd.DataFrame.from_dict(FIEBRE_TIFOIDEA_dict).transpose()\n",
    "data_df.columns = ['questions', 'answers']\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
